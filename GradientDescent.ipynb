{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment, you will use the gradient descent algorithm to find the optimal value of w1 which minimizes the given cost function (Steps 1 through 5).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_J(w1):\n",
    "    return (5.5*w1*w1 - 1463*w1 + 100812.5)\n",
    "\n",
    "# slope at particular value of w1\n",
    "def calc_dJ_dw1(w1):\n",
    "    return (11*w1 - 1463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial value of J is: 32832.5 & The initial value of w1 is: 60\n",
      "Iteration number 1\n",
      "----------value of w1---------\n",
      "124.24\n",
      "----------value of J----------\n",
      "3945.06\n",
      "Iteration number 2\n",
      "----------value of w1---------\n",
      "131.95\n",
      "----------value of J----------\n",
      "3529.08\n",
      "Iteration number 3\n",
      "----------value of w1---------\n",
      "132.87\n",
      "----------value of J----------\n",
      "3523.09\n",
      "Iteration number 4\n",
      "----------value of w1---------\n",
      "132.98\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 5\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 6\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 7\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 8\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 9\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 10\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 11\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 12\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 13\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 14\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 15\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 16\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 17\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 18\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 19\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 20\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 21\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 22\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 23\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 24\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n",
      "Iteration number 25\n",
      "----------value of w1---------\n",
      "133.0\n",
      "----------value of J----------\n",
      "3523.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha=0.08; niterations = 25; w1 = 60 # Scenario 1\n",
    "#alpha=0.05; niterations = 25; w1 = 60 # Scenario 2\n",
    "#alpha=0.15; niterations = 25; w1 = 60 # Scenario 3\n",
    "#alpha=0.19; niterations = 25; w1 = 60 # Scenario 4\n",
    "\n",
    "#Scenario 1\n",
    "#alpha=0.08; niterations = 25; w1 = 60 \n",
    "# Step 1: calculate current value of J at current value of w1\n",
    "# your code here\n",
    "J = calc_J(w1)\n",
    "#print(\"Old value J\", J)\n",
    "\n",
    "\n",
    "# Step 2: print w1 (rounded to 2 decimal places) and J (rounded to 2 decimal places)\n",
    "# your code here\n",
    "print(\"The initial value of J is:\", str(round(J, 2)), \"&\" ,\"The initial value of w1 is:\", str(round(w1, 2)))\n",
    "\n",
    "min_cost =[]\n",
    "\n",
    "# run gradient descent for niterations\n",
    "for i in range(niterations):\n",
    "    \n",
    "    # Step 3: calculate new value of w1\n",
    "    # your code here\n",
    "    w1 = w1 - (alpha)*(calc_dJ_dw1(w1))\n",
    "    \n",
    "    # Step 4: calculate new value of J at new w1\n",
    "    # your code here\n",
    "    J =  calc_J(w1)\n",
    "    # Step 5: print w1 (rounded to 2 decimal places) and J (rounded to 2 decimal places)\n",
    "    # your code here\n",
    "    print(\"Iteration number\", i+1)\n",
    "    print(\"----------value of w1---------\")\n",
    "    print(str(round(w1, 2)))\n",
    "    print(\"----------value of J----------\")\n",
    "    print(str(round(J, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: for each Scenario, answering the following questions based on the output\n",
    "# type your answers below for each question\n",
    "#\n",
    "### Scenario 1 (alpha=0.08; niterations = 25; w1 = 60):\n",
    "# did the cost steadily decrease as training progressed? \n",
    "# Yes, the cost decreased steadily\n",
    "# did the gradient descent algorithm converge? \n",
    "# Yes, the gradient descent algorithm converged\n",
    "# what is the minimum cost? \n",
    "#The minimum cost is 3523.0\n",
    "# what is the optimal value of w1? \n",
    "# The optimum value is 133.0\n",
    "# in how many iterations did gradient descent converge (at which row # in the output)? \n",
    "# 5 iterations\n",
    "\n",
    "### Scenario 2 (alpha=0.05; niterations = 25; w1 = 60):\n",
    "# did the cost steadily decrease as training progressed? \n",
    "# Yes, the cost decreased steadily\n",
    "# did the gradient descent algorithm converge? \n",
    "# Yes, the gradient descent algorithm converged\n",
    "# what is the minimum cost? \n",
    "#The minimum cost is 3523.0\n",
    "# what is the optimal value of w1? \n",
    "# The optimum value is 133.0\n",
    "# in how many iterations did gradient descent converge (at which row # in the output)? \n",
    "#11 iterations\n",
    "\n",
    "\n",
    "### Scenario 3 (alpha=0.15; niterations = 25; w1 = 60):\n",
    "# did the cost steadily decrease as training progressed? \n",
    "# Yes, the cost decreased steadily\n",
    "# did the gradient descent algorithm converge? \n",
    "# Yes, the gradient descent algorithm converged\n",
    "# what is the minimum cost? \n",
    "#The minimum cost is 3523.0\n",
    "# what is the optimal value of w1? \n",
    "# The optimum value is 133.0\n",
    "# in how many iterations did gradient descent converge (at which row # in the output)? \n",
    "#24 iterations \n",
    "\n",
    "\n",
    "### Scenario 4 (alpha=0.19; niterations = 25; w1 = 60):\n",
    "# did the cost steadily decrease as training progressed? \n",
    "#No, the cost increased steadily\n",
    "# did the gradient descent algorithm converge? \n",
    "#No, the algorithm did not converge, since there is no minimum stable value reached for W1 and J.\n",
    "# what is the minimum cost? \n",
    "#32832.5, which is our initial value\n",
    "# what is the optimal value of w1? \n",
    "# It is difficult to deduce the optimal value of W1, since the gradient descent never converges. \n",
    "# in how many iterations did gradient descent converge (at which row # in the output)? \n",
    "## It is difficult to deduce since the gradient descent never converges. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
